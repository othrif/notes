{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Basics of building a MLP\"\n",
    "date: 2020-04-12T14:41:32+02:00\n",
    "author: \"Othmane Rifki\"\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5458,  0.1715],\n",
       "        [ 0.0310, -0.2061],\n",
       "        [ 1.3804, -0.1380]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performs the operation  ùê¥ùë•+ùëè , where  ùê¥  and  ùëè  are initialized randomly\n",
    "linear = nn.Linear(10, 2) # nn.Linear(input dim=10, output dim=2) will take in a  ùëõ√ó10  matrix and return an  ùëõ√ó2  matrix \n",
    "example_input = torch.randn(3, 10)\n",
    "example_output = linear(example_input)\n",
    "example_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.3189],\n",
       "        [0.0000, 0.6577],\n",
       "        [0.1315, 0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReLU non-linearity sets all negative numbers in a tensor to zero\n",
    "relu = nn.ReLU()\n",
    "relu_output = relu(example_output)\n",
    "relu_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7062, -0.0247],\n",
       "        [-0.7062,  1.2368],\n",
       "        [ 1.4124, -1.2121]], grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rescale a batch of  ùëõ  inputs to have a consistent mean and standard deviation between batches\n",
    "batchnorm = nn.BatchNorm1d(2)\n",
    "batchnorm_output = batchnorm(relu_output)\n",
    "batchnorm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "tensor([[ 1.4355,  0.1143,  0.0974,  0.2137,  1.9092],\n",
      "        [-0.7489, -1.7915,  0.3816, -0.0109,  1.3555],\n",
      "        [-1.8546, -2.9966,  0.5250, -0.4175,  1.0728],\n",
      "        [ 0.5806, -0.6192, -0.0937, -1.3968,  0.8309],\n",
      "        [-1.2107, -0.3949,  0.8021, -0.7953, -0.5259]])\n",
      "output: \n",
      "tensor([[-0.8080,  1.9998],\n",
      "        [-0.8080, -0.5000],\n",
      "        [-0.8080, -0.5000],\n",
      "        [ 0.9855, -0.5000],\n",
      "        [ 1.4384, -0.5000]], grad_fn=<NativeBatchNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "# do it with one operation\n",
    "mlp_layer = nn.Sequential(\n",
    "    nn.Linear(5, 2),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(2)\n",
    "\n",
    ")\n",
    "\n",
    "test_example = torch.randn(5,5)\n",
    "print(\"input: \")\n",
    "print(test_example)\n",
    "print(\"output: \")\n",
    "print(mlp_layer(test_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "adam_opt = optim.Adam(mlp_layer.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5653, grad_fn=<MeanBackward0>)\n",
      "tensor(0.4999, grad_fn=<MeanBackward0>)\n",
      "tensor(0.4354, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3699, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3009, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2271, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1474, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0749, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0687, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1651, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_example = torch.randn(100,5) + 1\n",
    "adam_opt.zero_grad()\n",
    "\n",
    "for i in range(10):  \n",
    "    # Loss function\n",
    "    cur_loss = torch.abs(1 - mlp_layer(train_example)).mean()\n",
    "    cur_loss.backward()\n",
    "    adam_opt.step()\n",
    "    print(cur_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
